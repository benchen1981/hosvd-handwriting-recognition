"""
ğŸ“ ä¸»ç¨‹å¼ - HOSVD æ‰‹å¯«æ•¸å­—è­˜åˆ¥ç³»çµ±
============================================
æƒ³åƒé€™å€‹ç¨‹å¼æ˜¯ä¸€å€‹å®Œæ•´çš„å¯¦é©—å®¤å·¥ä½œæµ:
1. æ‹¿å‡ºæ‰€æœ‰å·¥å…·å’Œææ–™ (æº–å‚™æ•¸æ“š)
2. é€²è¡Œå¯¦é©— (è¨“ç·´æ¨¡å‹)
3. æ¸¬è©¦çµæœ (è©•ä¼°æ¨¡å‹)
4. æ‹ç…§è¨˜éŒ„ (ç”Ÿæˆåœ–è¡¨)
5. å¯«ä¸‹ä¾†å ±å‘Š (ä¿å­˜çµæœ)

ä½œè€…: é™³å®¥èˆˆ (5114050015)
"""

# ==================== ç¬¬1æ­¥: æº–å‚™æ‰€æœ‰å·¥å…· ====================
# å°±åƒå¯¦é©—é–‹å§‹å‰è¦æº–å‚™å„ç¨®å„€å™¨å’Œè©¦åŠ‘

import os              # æ–‡ä»¶å’Œè·¯å¾‘ç®¡ç†
import sys              # ç³»çµ±åŠŸèƒ½
import argparse        # å‘½ä»¤è¡Œåƒæ•¸è§£æ
import numpy as np     # æ•¸å­¸è¨ˆç®—
import matplotlib.pyplot as plt  # ç•«åœ–å·¥å…·

from datetime import datetime  # æ™‚é–“åŠŸèƒ½

# å°å…¥æˆ‘å€‘è‡ªå·±å¯«çš„æ¨¡çµ„
from config import DATA_CONFIG, HOSVD_CONFIG, CLASSIFIER_CONFIG, PATH_CONFIG, VIZ_CONFIG  # é…ç½®
from data import load_data, DataPreprocessor  # æ•¸æ“šè¼‰å…¥å’Œæº–å‚™
from models import HOSVDModel, ClassifierPipeline  # æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
from utils import (
    Metrics,                    # è©•ä¼°æŒ‡æ¨™è¨ˆç®—
    ModelEvaluator,             # æ¨¡å‹è©•ä¼°å™¨
    FileManager,                # æ–‡ä»¶ç®¡ç†
    Logger,                     # æ—¥èªŒè¨˜éŒ„
    plot_digits,                # ç•«æ•¸å­—å‡½æ•¸
    plot_confusion_matrix,      # ç•«æ··æ·†çŸ©é™£
    plot_classification_metrics, # ç•«åˆ†é¡æŒ‡æ¨™
    plot_dimensionality_reduction # ç•«é™ç¶­åœ–
)


# ==================== ç¬¬2æ­¥: è¨­ç½®æ—¥èªŒ ====================
# æ—¥èªŒå°±åƒè¨˜è€…è¨˜éŒ„æ•´å€‹å¯¦é©—éç¨‹

def setup_logging():
    """
    è¨­ç½®æ—¥èªŒç³»çµ±ã€‚
    
    æƒ³åƒéç¨‹:
    å°±åƒé–‹å§‹å¯«æ—¥è¨˜,è¨˜éŒ„æ‰€æœ‰ç™¼ç”Ÿçš„äº‹æƒ…
    """
    logger = Logger.setup_logger("HOSVD_System")  # å»ºç«‹æ—¥èªŒè¨˜éŒ„å™¨
    return logger


# ==================== ç¬¬3æ­¥: å‰µå»ºè¼¸å‡ºç›®éŒ„ ====================
# å°±åƒåšå¯¦é©—å‰è¦æº–å‚™å¥½æ”¾çµæœçš„æ–‡ä»¶å¤¾

def create_directories():
    """
    å‰µå»ºæ‰€æœ‰å¿…è¦çš„è¼¸å‡ºç›®éŒ„ã€‚
    
    æƒ³åƒéç¨‹:
    1. çœ‹çœ‹éœ€è¦å“ªäº›æ–‡ä»¶å¤¾
    2. å¦‚æœæ–‡ä»¶å¤¾ä¸å­˜åœ¨,å°±å»ºç«‹å®ƒ
    3. æº–å‚™å¥½æ”¾çµæœ
    """
    for path in PATH_CONFIG.values():  # å°æ¯å€‹è·¯å¾‘
        os.makedirs(path, exist_ok=True)  # å»ºç«‹æ–‡ä»¶å¤¾ (å¦‚æœä¸å­˜åœ¨)


# ==================== ç¬¬4æ­¥: åŠ è¼‰å’Œæº–å‚™æ•¸æ“š ====================
# å°±åƒåšèœå…ˆè¦æº–å‚™é£Ÿæ

def load_and_preprocess_data(dataset='mnist', normalize=True):
    """
    åŠ è¼‰å’Œé è™•ç†æ•¸æ“šã€‚
    
    æƒ³åƒéç¨‹:
    1. æ‰“é–‹åŒ…å«æ•¸æ“šçš„æ–‡ä»¶ (åƒæ‰“é–‹é£Ÿè­œ)
    2. æª¢æŸ¥æ•¸æ“šçš„å¤§å° (åƒç¨±é‡é£Ÿæ)
    3. æ¸…ç†å’Œæ•´ç†æ•¸æ“š (åƒæ´—èœå’Œåˆ‡èœ)
    4. æ¸¬è©¦æ•¸æ“šè³ªé‡ (åƒåšå‘³é“)
    
    åƒæ•¸:
        dataset: æ•¸æ“šé›†çš„åå­— ('mnist' æ˜¯æ‰‹å¯«æ•¸å­—åœ–ç‰‡)
        normalize: æ˜¯å¦è¦æŠŠæ•¸æ“šæ¨™æº–åŒ–åˆ° 0~1 ä¹‹é–“
    
    å›å‚³:
        4 å€‹æ±è¥¿: è¨“ç·´åœ–ç‰‡ã€è¨“ç·´æ¨™ç±¤ã€æ¸¬è©¦åœ–ç‰‡ã€æ¸¬è©¦æ¨™ç±¤
    """
    logger = Logger.setup_logger(__name__)  # å»ºç«‹æ—¥èªŒè¨˜éŒ„å™¨
    
    # æ‰“é–‹æ•¸æ“š (å°±åƒæ‰“é–‹é£ŸæåŒ…)
    logger.info(f"Loading {dataset} dataset...")
    X_train, y_train, X_test, y_test = load_data(dataset, normalize=normalize)
    
    # é¡¯ç¤ºæ•¸æ“šçš„å¤§å°
    logger.info(f"Original shapes - Train: {X_train.shape}, Test: {X_test.shape}")
    # ä¾‹å¦‚: Original shapes - Train: (60000, 784), Test: (10000, 784)
    # æ„æ€æ˜¯: 60000 å¼µè¨“ç·´åœ–ç‰‡,10000 å¼µæ¸¬è©¦åœ–ç‰‡,æ¯å¼µ 784 å€‹åƒç´ 
    
    # åˆå§‹åŒ–é è™•ç†å·¥å…· (å°±åƒæº–å‚™çƒ¹é£ªå·¥å…·)
    preprocessor = DataPreprocessor(normalize=normalize, standardize=False)
    
    # å°è¨“ç·´æ•¸æ“šé€²è¡Œé è™•ç† (å°±åƒæ´—èœ)
    X_train_processed = preprocessor.fit_transform(X_train)
    
    # å°æ¸¬è©¦æ•¸æ“šé€²è¡Œé è™•ç† (å°±åƒå†æ´—ä¸€éèœ)
    X_test_processed = preprocessor.transform(X_test)
    
    # é¡¯ç¤ºé è™•ç†å¾Œçš„å¤§å°
    logger.info(f"Preprocessed shapes - Train: {X_train_processed.shape}, Test: {X_test_processed.shape}")
    
    # å›å‚³æ‰€æœ‰æ±è¥¿
    return X_train_processed, y_train, X_test_processed, y_test


# ==================== ç¬¬5æ­¥: æ‡‰ç”¨ HOSVD é™ç¶­ ====================
# å°±åƒæŠŠèœçš„ç‡Ÿé¤Šå£“ç¸®åˆ°æ›´å°çš„ç©ºé–“

def apply_hosvd(X_train, X_test, n_components=50):
    """
    æ‡‰ç”¨ HOSVD æ¼”ç®—æ³•é€²è¡Œé™ç¶­ã€‚
    
    æƒ³åƒéç¨‹:
    1. å»ºç«‹ HOSVD å·¥å…· (å°±åƒè²·ä¸€å€‹å£“ç¸®æ©Ÿ)
    2. è¨“ç·´é€™å€‹å·¥å…· (å°±åƒå­¸æœƒå¦‚ä½•æ“ä½œ)
    3. å°è¨“ç·´æ•¸æ“šå£“ç¸® (å°±åƒå£“ç¸®è¨“ç·´èœ)
    4. å°æ¸¬è©¦æ•¸æ“šå£“ç¸® (å°±åƒå£“ç¸®æ¸¬è©¦èœ)
    5. æª¢æŸ¥å£“ç¸®ç‡ (å°±åƒçœ‹èƒ½çœå¤šå°‘ç©ºé–“)
    
    åƒæ•¸:
        X_train: è¨“ç·´åœ–ç‰‡æ•¸æ“š
        X_test: æ¸¬è©¦åœ–ç‰‡æ•¸æ“š
        n_components: è¦ä¿ç•™å¤šå°‘å€‹ä¸»è¦ç‰¹å¾µ (50 = ä¿ç•™ 50 å€‹æœ€é‡è¦çš„ç‰¹å¾µ)
    
    å›å‚³:
        å£“ç¸®å¾Œçš„è¨“ç·´æ•¸æ“šã€å£“ç¸®å¾Œçš„æ¸¬è©¦æ•¸æ“šã€HOSVD æ¨¡å‹
    """
    logger = Logger.setup_logger(__name__)
    
    # å‘Šè¨´ç”¨æˆ¶æ­£åœ¨é€²è¡Œé™ç¶­
    logger.info(f"Applying HOSVD with {n_components} components...")
    
    # å»ºç«‹ HOSVD æ¨¡å‹ (å°±åƒå»ºç«‹å£“ç¸®æ©Ÿ)
    hosvd = HOSVDModel(n_components=n_components)
    
    # è¨“ç·´å’Œå£“ç¸®è¨“ç·´æ•¸æ“š (å°±åƒå£“ç¸®è¨“ç·´èœ)
    X_train_reduced = hosvd.fit_transform(X_train)
    
    # ç”¨å·²å­¸æœƒçš„æ–¹æ³•å£“ç¸®æ¸¬è©¦æ•¸æ“š (å°±åƒç”¨åŒæ¨£çš„æ–¹æ³•å£“ç¸®æ¸¬è©¦èœ)
    X_test_reduced = hosvd.transform(X_test)
    
    # é¡¯ç¤ºå£“ç¸®å¾Œçš„å¤§å°
    logger.info(f"Reduced shapes - Train: {X_train_reduced.shape}, Test: {X_test_reduced.shape}")
    # ä¾‹å¦‚: Reduced shapes - Train: (60000, 50), Test: (10000, 50)
    # æ„æ€æ˜¯: å¾ 784 å€‹ç‰¹å¾µé™åˆ° 50 å€‹!
    
    # é¡¯ç¤ºæ ¸å¿ƒå¼µé‡çš„å¤§å°
    logger.info(f"Core tensor shape: {hosvd.get_core_tensor_shape()}")
    
    # é¡¯ç¤ºå£“ç¸®ç‡ (èƒ½ç¯€çœå¤šå°‘ç©ºé–“)
    logger.info(f"Compression ratio: {hosvd.get_compression_ratio():.4f}")
    # ä¾‹å¦‚: 0.0638 = å£“ç¸®åˆ°åŸä¾†å¤§å°çš„ 6.38%
    
    # æŠŠæ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶ (å°±åƒå‚™ä»½å£“ç¸®æ©Ÿçš„é…ç½®)
    model_path = os.path.join(PATH_CONFIG['model_dir'], 'hosvd_model.pkl')
    FileManager.save_model(hosvd, model_path)
    
    return X_train_reduced, X_test_reduced, hosvd


# ==================== ç¬¬6æ­¥: è¨“ç·´åˆ†é¡å™¨ ====================
# å°±åƒè¨“ç·´ä¸€å€‹å»šå¸«èªå‡ºèœçš„åå­—

def train_classifier(X_train, y_train, classifier_type='knn', **kwargs):
    """
    è¨“ç·´åˆ†é¡å™¨ã€‚
    
    æƒ³åƒéç¨‹:
    1. é¸æ“‡ä¸€å€‹åˆ†é¡å™¨ (å°±åƒé¸æ“‡ä¸€å€‹å­¸ç”Ÿ)
    2. çµ¦ä»–çœ‹è¨“ç·´åœ–ç‰‡å’Œæ¨™ç±¤ (å°±åƒæ•™ä»–èªå‡ºèœ)
    3. å­¸ç”Ÿé‡è¤‡å­¸ç¿’ç›´åˆ°èƒ½æ­£ç¢ºé æ¸¬ (å°±åƒåè¤‡ç·´ç¿’)
    4. æ¸¬è©¦å­¸ç”Ÿåœ¨è¨“ç·´é›†ä¸Šçš„æº–ç¢ºç‡
    5. ä¿å­˜é€™å€‹è¨“ç·´å¥½çš„å­¸ç”Ÿ (å°±åƒè¨˜éŒ„ä»–çš„çŸ¥è­˜)
    
    åƒæ•¸:
        X_train: è¨“ç·´åœ–ç‰‡ (å£“ç¸®å¾Œ)
        y_train: è¨“ç·´æ¨™ç±¤ (æ­£ç¢ºç­”æ¡ˆ)
        classifier_type: åˆ†é¡å™¨é¡å‹ ('knn', 'svm', 'rf', æˆ– 'mlp')
        **kwargs: åˆ†é¡å™¨çš„åƒæ•¸è¨­å®š
    
    å›å‚³:
        è¨“ç·´å¥½çš„åˆ†é¡å™¨
    """
    logger = Logger.setup_logger(__name__)
    
    # å‘Šè¨´ç”¨æˆ¶æ­£åœ¨è¨“ç·´
    logger.info(f"Training {classifier_type} classifier...")
    
    # å»ºç«‹åˆ†é¡å™¨ (å°±åƒæ‹›è˜ä¸€å€‹æ–°å­¸ç”Ÿ)
    classifier = ClassifierPipeline(classifier_type, **kwargs)
    
    # è¨“ç·´åˆ†é¡å™¨ (å°±åƒæ•™ä»–)
    classifier.fit(X_train, y_train)
    
    # æ¸¬è©¦åˆ†é¡å™¨åœ¨è¨“ç·´é›†ä¸Šçš„æº–ç¢ºç‡
    train_accuracy = classifier.score(X_train, y_train)
    logger.info(f"Train accuracy: {train_accuracy:.4f}")
    # ä¾‹å¦‚: Train accuracy: 0.9752 = 97.52% çš„æº–ç¢ºç‡
    
    # ä¿å­˜è¨“ç·´å¥½çš„åˆ†é¡å™¨ (å°±åƒå‚™ä»½ä»–çš„çŸ¥è­˜)
    model_path = os.path.join(PATH_CONFIG['model_dir'], f'{classifier_type}_classifier.pkl')
    FileManager.save_model(classifier, model_path)
    
    return classifier


# ==================== ç¬¬7æ­¥: è©•ä¼°æ¨¡å‹ ====================
# å°±åƒç”¨æ¸¬è©¦é¡Œè€ƒå­¸ç”Ÿ

def evaluate_model(classifier, X_test, y_test, dataset_name=""):
    """
    è©•ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
    
    æƒ³åƒéç¨‹:
    1. ç”¨æ¸¬è©¦æ•¸æ“šè€ƒå­¸ç”Ÿ (æ²’çœ‹éçš„é¡Œç›®)
    2. è¨ˆç®—ä»–çš„æº–ç¢ºç‡ã€ç²¾ç¢ºåº¦ã€å¬å›ç‡ç­‰
    3. è£½ä½œæ··æ·†çŸ©é™£ (çœ‹ä»–å®¹æ˜“æŠŠå“ªäº›æ•¸å­—ææ··)
    4. è¨˜éŒ„æ‰€æœ‰çµæœ
    
    åƒæ•¸:
        classifier: è¨“ç·´å¥½çš„åˆ†é¡å™¨
        X_test: æ¸¬è©¦åœ–ç‰‡ (å£“ç¸®å¾Œ)
        y_test: æ¸¬è©¦æ¨™ç±¤ (æ­£ç¢ºç­”æ¡ˆ)
        dataset_name: æ•¸æ“šé›†çš„åå­— (ç”¨æ–¼é¡¯ç¤º)
    
    å›å‚³:
        è©•ä¼°çµæœ (åŒ…å«æ‰€æœ‰æŒ‡æ¨™å’Œæ··æ·†çŸ©é™£)
    """
    logger = Logger.setup_logger(__name__)
    
    # å‘Šè¨´ç”¨æˆ¶æ­£åœ¨è©•ä¼°
    logger.info(f"\nEvaluating model on {dataset_name}...")
    
    # é æ¸¬ (å­¸ç”Ÿå›ç­”æ‰€æœ‰æ¸¬è©¦é¡Œ)
    y_pred = classifier.predict(X_test)
    
    # å»ºç«‹è©•ä¼°å™¨ (å°±åƒè©•åˆ†è€å¸«)
    evaluator = ModelEvaluator(y_test, y_pred)
    
    # è¨ˆç®—æ‰€æœ‰è©•ä¼°æŒ‡æ¨™
    metrics = evaluator.get_metrics()
    
    # é¡¯ç¤ºçµæœ
    logger.info(f"Test accuracy: {metrics['accuracy']:.4f}")
    logger.info(f"Precision: {metrics['precision']:.4f}")
    logger.info(f"Recall: {metrics['recall']:.4f}")
    logger.info(f"F1-Score: {metrics['f1']:.4f}")
    
    return evaluator


# ==================== ç¬¬8æ­¥: ç”Ÿæˆåœ–è¡¨ ====================
# å°±åƒæŠŠå¯¦é©—çµæœç•«æˆåœ–è¡¨

def generate_visualizations(X_train, y_train, X_test, y_test, X_test_reduced, 
                          evaluator, classifier_type, output_dir):
    """
    ç”Ÿæˆå„ç¨®åœ–è¡¨å’Œåœ–ç‰‡ã€‚
    
    æƒ³åƒéç¨‹:
    1. ç•«å‡ºä¸€äº›æ¨£æœ¬æ•¸å­— (çœ‹çœ‹æ•¸æ“šé•·ä»€éº¼æ¨£)
    2. ç•«æ··æ·†çŸ©é™£ (çœ‹æ¨¡å‹å®¹æ˜“å‡ºéŒ¯çš„åœ°æ–¹)
    3. ç•«é™ç¶­æ•ˆæœ (çœ‹å£“ç¸®å¾Œçš„æ•ˆæœ)
    4. ç•«åˆ†é¡æŒ‡æ¨™ (çœ‹æ€§èƒ½æŒ‡æ¨™)
    5. ä¿å­˜æ‰€æœ‰åœ–ç‰‡
    
    åƒæ•¸:
        X_train, y_train: è¨“ç·´åœ–ç‰‡å’Œæ¨™ç±¤
        X_test, y_test: æ¸¬è©¦åœ–ç‰‡å’Œæ¨™ç±¤
        X_test_reduced: å£“ç¸®å¾Œçš„æ¸¬è©¦åœ–ç‰‡
        evaluator: è©•ä¼°å™¨ (å«è©•ä¼°çµæœ)
        classifier_type: åˆ†é¡å™¨é¡å‹ (ç”¨æ–¼å‘½åæ–‡ä»¶)
        output_dir: è¼¸å‡ºåœ–ç‰‡çš„æ–‡ä»¶å¤¾
    """
    logger = Logger.setup_logger(__name__)
    
    logger.info("Generating visualizations...")  # å‘Šè¨´ç”¨æˆ¶æ­£åœ¨ç”Ÿæˆåœ–è¡¨
    
    # ç”¨æ™‚é–“æˆ³ä½œç‚ºæ–‡ä»¶å (æ¯æ¬¡é‹è¡Œéƒ½ä¸åŒ,ä¸æœƒè¦†è“‹)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # ä¾‹å¦‚: 20250103_143022
    
    # ====== åœ–è¡¨ 1: æ¨£æœ¬æ•¸å­— ======
    # å°±åƒå±•ç¤ºä½ æ”¶é›†çš„æ•¸æ“šæ¨£æœ¬
    fig = plot_digits(X_test[:25], y_test[:25], n_rows=5, n_cols=5, 
                     title="Sample Test Digits")
    fig.savefig(os.path.join(output_dir, f'sample_digits_{timestamp}.png'), dpi=100)
    plt.close(fig)  # é—œé–‰åœ–è¡¨,ç¯€çœè¨˜æ†¶é«”
    
    # ====== åœ–è¡¨ 2: æ··æ·†çŸ©é™£ ======
    # 10x10 çš„è¡¨æ ¼,é¡¯ç¤ºæ¨¡å‹æŠŠå“ªäº›æ•¸å­—ææ··
    cm = evaluator.get_confusion_matrix()
    fig = plot_confusion_matrix(cm)
    fig.savefig(os.path.join(output_dir, f'confusion_matrix_{classifier_type}_{timestamp}.png'), dpi=100)
    plt.close(fig)
    
    # ====== åœ–è¡¨ 3: é™ç¶­æ•ˆæœ ======
    # ç”¨ PCA æŠŠé™ç¶­å¾Œçš„æ•¸æ“šç•«æˆ 2D åœ–,çœ‹çœ‹åˆ†ä½ˆæƒ…æ³
    try:
        fig = plot_dimensionality_reduction(X_test, X_test_reduced, y_test)
        fig.savefig(os.path.join(output_dir, f'dimensionality_reduction_{timestamp}.png'), dpi=100)
        plt.close(fig)
    except Exception as e:
        logger.warning(f"Could not generate dimensionality reduction plot: {e}")
    
    # ====== åœ–è¡¨ 4: åˆ†é¡æŒ‡æ¨™ ======
    # ç”¨æŸ±ç‹€åœ–é¡¯ç¤ºæº–ç¢ºç‡ã€ç²¾ç¢ºåº¦ç­‰æŒ‡æ¨™
    metrics = evaluator.get_metrics()
    fig, ax = plt.subplots(figsize=(8, 6))
    metric_names = list(metrics.keys())       # æŒ‡æ¨™åå­—
    metric_values = list(metrics.values())    # æŒ‡æ¨™å€¼
    
    # ç•«æŸ±ç‹€åœ–
    ax.barh(metric_names, metric_values, color='skyblue')
    ax.set_xlabel('Score', fontsize=12)
    ax.set_title(f'Classification Metrics ({classifier_type})', fontsize=14)
    ax.set_xlim([0, 1.1])  # é™åˆ¶ x è»¸ç¯„åœ (0 åˆ° 1.1)
    
    # åœ¨æ¯å€‹æŸ±å­ä¸Šå¯«ä¸Šæ•¸å€¼
    for i, v in enumerate(metric_values):
        ax.text(v + 0.01, i, f'{v:.4f}', va='center')
    
    ax.grid(axis='x', alpha=0.3)  # åŠ ä¸Šç¶²æ ¼ç·š
    plt.tight_layout()
    fig.savefig(os.path.join(output_dir, f'metrics_{classifier_type}_{timestamp}.png'), dpi=100)
    plt.close(fig)
    
    logger.info(f"Visualizations saved to {output_dir}")


# ==================== ç¬¬9æ­¥: ä¸»å‡½æ•¸ ====================
# é€™æ˜¯æ•´å€‹ç¨‹åºçš„æ§åˆ¶ä¸­å¿ƒ

def main(args):
    """
    ä¸»å‡½æ•¸ - å”èª¿æ•´å€‹å¯¦é©—æµç¨‹ã€‚
    
    æƒ³åƒéç¨‹:
    é€™å°±åƒä¸€å€‹å¯¦é©—å®¤ä¸»ä»»,æŒ‡æ®æ‰€æœ‰æ­¥é©Ÿ:
    1. æº–å‚™å·¥ä½œ (å»ºç«‹ç›®éŒ„,è¨­ç½®æ—¥èªŒ)
    2. ç¬¬ä¸€æ­¥: åŠ è¼‰æ•¸æ“š
    3. ç¬¬äºŒæ­¥: é™ç¶­
    4. ç¬¬ä¸‰æ­¥: è¨“ç·´æ¨¡å‹
    5. ç¬¬å››æ­¥: è©•ä¼°æ¨¡å‹
    6. ç¬¬äº”æ­¥: ç”Ÿæˆåœ–è¡¨
    7. ç¬¬å…­æ­¥: ä¿å­˜çµæœ
    
    åƒæ•¸:
        args: å‘½ä»¤è¡Œåƒæ•¸ (ä¾‹å¦‚ä½¿ç”¨å“ªå€‹æ•¸æ“šé›†ã€å¤šå°‘å€‹çµ„ä»¶ç­‰)
    """
    # ====== æº–å‚™å·¥ä½œ ======
    logger = setup_logging()  # é–‹å§‹è¨˜éŒ„
    create_directories()      # å»ºç«‹è¼¸å‡ºæ–‡ä»¶å¤¾
    
    # é¡¯ç¤ºæ¨™é¡Œå’Œé…ç½®ä¿¡æ¯
    logger.info("=" * 80)
    logger.info("HOSVD Handwriting Recognition System")
    logger.info("=" * 80)
    logger.info(f"Configuration:")
    logger.info(f"  Dataset: {args.dataset}")
    logger.info(f"  HOSVD components: {args.n_components}")
    logger.info(f"  Classifier: {args.classifier}")
    logger.info(f"  Test size: {args.test_size}")
    logger.info("=" * 80)
    
    # ====== ç¬¬ 1 æ­¥: åŠ è¼‰å’Œæº–å‚™æ•¸æ“š ======
    X_train, y_train, X_test, y_test = load_and_preprocess_data(
        dataset=args.dataset,
        normalize=True
    )
    
    # ====== ç¬¬ 2 æ­¥: æ‡‰ç”¨ HOSVD é™ç¶­ ======
    X_train_reduced, X_test_reduced, hosvd_model = apply_hosvd(
        X_train, X_test,
        n_components=args.n_components
    )
    
    # ====== ç¬¬ 3 æ­¥: è¨“ç·´åˆ†é¡å™¨ ======
    classifier_kwargs = CLASSIFIER_CONFIG.get(args.classifier, {})
    classifier = train_classifier(
        X_train_reduced, y_train,
        classifier_type=args.classifier,
        **classifier_kwargs
    )
    
    # ====== ç¬¬ 4 æ­¥: è©•ä¼°æ¨¡å‹ ======
    evaluator = evaluate_model(
        classifier, X_test_reduced, y_test,
        dataset_name=args.dataset
    )
    
    # ====== ç¬¬ 5 æ­¥: ç”Ÿæˆåœ–è¡¨ ======
    if args.visualize:  # å¦‚æœç”¨æˆ¶è¦æ±‚ç”Ÿæˆåœ–è¡¨
        generate_visualizations(
            X_train, y_train, X_test, y_test, X_test_reduced,
            evaluator, args.classifier, PATH_CONFIG['figure_dir']
        )
    
    # ====== ç¬¬ 6 æ­¥: ä¿å­˜çµæœ ======
    results = {
        'timestamp': datetime.now().isoformat(),  # æ™‚é–“æˆ³
        'configuration': {  # é…ç½®ä¿¡æ¯
            'dataset': args.dataset,
            'n_components': args.n_components,
            'classifier': args.classifier,
            'test_size': args.test_size,
        },
        'hosvd_info': {  # HOSVD ä¿¡æ¯
            'core_tensor_shape': hosvd_model.get_core_tensor_shape(),
            'compression_ratio': float(hosvd_model.get_compression_ratio()),
        },
        'metrics': evaluator.get_metrics(),  # è©•ä¼°æŒ‡æ¨™
    }
    
    # ä¿å­˜çµæœåˆ° JSON æ–‡ä»¶
    result_path = os.path.join(PATH_CONFIG['model_dir'], 'results.json')
    FileManager.save_json(results, result_path)
    
    # é¡¯ç¤ºå®Œæˆæ¶ˆæ¯
    logger.info("=" * 80)
    logger.info("Experiment completed successfully!")
    logger.info(f"Results saved to {result_path}")
    logger.info("=" * 80)
    
    return results  # å›å‚³çµæœ


# ==================== ç¬¬ 10 æ­¥: å‘½ä»¤è¡Œåƒæ•¸è§£æ ====================
# è®“ç”¨æˆ¶å¯ä»¥è‡ªè¨‚ç¨‹åºçš„è¡Œç‚º

if __name__ == "__main__":  # åªæœ‰ç›´æ¥é‹è¡Œé€™å€‹æ–‡ä»¶æ™‚æ‰åŸ·è¡Œ
    # å»ºç«‹å‘½ä»¤è¡Œåƒæ•¸è§£æå™¨ (å°±åƒå¯«ä½¿ç”¨èªªæ˜æ›¸)
    parser = argparse.ArgumentParser(
        description="HOSVD Handwriting Recognition System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹å­:
  # åŸºæœ¬ç”¨æ³•
  python main.py
  
  # ä½¿ç”¨ Fashion-MNIST æ•¸æ“šé›†
  python main.py --dataset fashion_mnist
  
  # ä½¿ç”¨ SVM åˆ†é¡å™¨
  python main.py --classifier svm --n_components 100
  
  # ä½¿ç”¨éš¨æ©Ÿæ£®æ—,æ›´å¤šçµ„ä»¶
  python main.py --classifier rf --n_components 150
        """
    )
    
    # å®šç¾©å„å€‹åƒæ•¸
    parser.add_argument('--dataset', type=str, default='mnist',
                       choices=['mnist', 'fashion_mnist', 'digits'],
                       help='ä½¿ç”¨å“ªå€‹æ•¸æ“šé›† (default: mnist)')
    
    parser.add_argument('--n_components', type=int, default=50,
                       help='HOSVD çš„çµ„ä»¶æ•¸é‡ (default: 50)')
    
    parser.add_argument('--classifier', type=str, default='knn',
                       choices=['knn', 'svm', 'rf', 'mlp'],
                       help='åˆ†é¡å™¨é¡å‹ (default: knn)')
    
    parser.add_argument('--test_size', type=float, default=0.2,
                       help='æ¸¬è©¦é›†çš„æ¯”ä¾‹ (default: 0.2 = 20%)')
    
    parser.add_argument('--no-visualize', dest='visualize', action='store_false',
                       help='ä¸ç”Ÿæˆåœ–è¡¨')
    
    # è¨­å®šé»˜èªå€¼
    parser.set_defaults(visualize=True)
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    args = parser.parse_args()
    
    # ====== é‹è¡Œä¸»ç¨‹åº ======
    results = main(args)
    
    # ====== æ‰“å°çµæœæ‘˜è¦ ======
    # é€™å°±åƒå¯¦é©—å ±å‘Šçš„æœ€å¾Œä¸€é ,ç¸½çµæ‰€æœ‰é‡è¦çµæœ
    print("\n" + "=" * 80)
    print("RESULTS SUMMARY (çµæœæ‘˜è¦)")
    print("=" * 80)
    print(f"Dataset: {results['configuration']['dataset']}")
    print(f"Classifier: {results['configuration']['classifier']}")
    print(f"Accuracy: {results['metrics']['accuracy']:.4f}")  # æ­£ç¢ºç‡
    print(f"Precision: {results['metrics']['precision']:.4f}")  # ç²¾ç¢ºåº¦
    print(f"Recall: {results['metrics']['recall']:.4f}")  # å¬å›ç‡
    print(f"F1-Score: {results['metrics']['f1']:.4f}")  # F1 åˆ†æ•¸
    print(f"Compression Ratio: {results['hosvd_info']['compression_ratio']:.4f}")  # å£“ç¸®ç‡
    print("=" * 80)

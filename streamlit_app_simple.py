"""
ğŸ”¢ ç°¡åŒ–ç‰ˆ Streamlit Web æ‡‰ç”¨ - æ‰‹å¯«æ•¸å­—è­˜åˆ¥ç³»çµ± (Python 3.13 ç›¸å®¹ç‰ˆæœ¬)
ç‰ˆæœ¬: 3.0
"""

import os
import sys
import pickle
import numpy as np
import streamlit as st
from PIL import Image
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# è·¯å¾‘è¨­ç½®
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from data import load_data, DataPreprocessor
    from models import HOSVDModel, ClassifierPipeline
except ImportError as e:
    st.error(f"Import Error: {e}")
    st.stop()

# ==================== é é¢é…ç½® ====================
st.set_page_config(
    page_title="HOSVD æ‰‹å¯«æ•¸å­—è­˜åˆ¥ç³»çµ±",
    page_icon="ğŸ”¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main { padding: 0rem 1rem; }
    h1 { color: #667eea; text-align: center; }
    h2 { color: #667eea; border-bottom: 2px solid #667eea; padding-bottom: 10px; }
    h3 { color: #764ba2; }
    .metric-card { background: #f0f2f6; padding: 15px; border-radius: 10px; margin: 10px 0; }
</style>
""", unsafe_allow_html=True)

# ==================== å¿«å–å‡½æ•¸ ====================
@st.cache_resource
def load_model_and_data():
    """è¼‰å…¥æ¨¡å‹å’Œæ•¸æ“š"""
    try:
        model_path = 'results/models/hosvd_model_latest.pkl'
        
        if os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
        else:
            st.info("â³ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨å¿«é€Ÿè¨“ç·´æ¨¡å‹...")
            try:
                X_train, y_train, _, _ = load_data('mnist', normalize=True)
            except:
                X_train, y_train, _, _ = load_data('digits', normalize=True)
            
            X_train = X_train[:5000]
            y_train = y_train[:5000]
            model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
            model.fit(X_train, y_train)
            # Save trained model for future runs so user won't see training message again
            model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results', 'models')
            os.makedirs(model_dir, exist_ok=True)
            model_path = os.path.join(model_dir, 'hosvd_model_latest.pkl')
            try:
                with open(model_path, 'wb') as mf:
                    pickle.dump(model, mf)
                st.success("âœ… æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å·²å„²å­˜ (saved).")
                st.session_state['model_trained'] = True
            except Exception as e:
                st.warning(f"æ¨¡å‹è¨“ç·´å®Œæˆï¼Œä½†å„²å­˜å¤±æ•—: {e}")
        
        preprocessor = DataPreprocessor()
        
        try:
            X_train, y_train, X_test, y_test = load_data('mnist', normalize=True)
            X_train = X_train[:10000]
            y_train = y_train[:10000]
            X_test = X_test[:2000]
            y_test = y_test[:2000]
        except:
            try:
                X_train, y_train, X_test, y_test = load_data('digits', normalize=True)
            except:
                X_train, y_train, X_test, y_test = None, None, None, None
        
        return model, preprocessor, X_train, y_train, X_test, y_test
        
    except Exception as e:
        st.error(f"Model loading failed: {e}")
        return None, None, None, None, None, None

def preprocess_image(image, size=(28, 28)):
    """åœ–ç‰‡å‰è™•ç†"""
    try:
        img = image.convert('L')
        img = img.resize(size)
        img_array = np.array(img, dtype=np.float32)
        img_array = 255 - img_array
        img_array = img_array / 255.0
        return img_array.flatten().reshape(1, -1)
    except:
        return None

def predict_digit(model, image_array):
    """é æ¸¬æ•¸å­—"""
    try:
        prediction = model.predict(image_array)[0]
        probabilities = model.predict_proba(image_array)[0]
        confidence = probabilities[prediction]
        return int(prediction), float(confidence), probabilities
    except:
        return None, None, None

# ==================== è¼‰å…¥æ¨¡å‹å’Œæ•¸æ“š ====================
model, preprocessor, X_train, y_train, X_test, y_test = load_model_and_data()

st.title("ğŸ”¢ HOSVD æ‰‹å¯«æ•¸å­—è­˜åˆ¥ç³»çµ±")
st.markdown("é«˜éšå¥‡ç•°å€¼åˆ†è§£ + å¤šåˆ†é¡å™¨é›†æˆ")

if model is None:
    st.error("âŒ Model Loading Failed")
    st.stop()

# ==================== å´é‚Šæ¬„å°èˆª ====================
page = st.sidebar.radio(
    "ğŸ“‹ Menu",
    ["ğŸ  Home", "ğŸ“š Dataset Info", "ğŸ¨ Draw Digit", "ğŸ“¸ Upload Image", "ğŸ“Š Batch Test", "ğŸ“ˆ Model Evaluation", "ğŸ”¬ Model Training", "ğŸ“Š Performance Comparison"]
)

# ==================== é¦–é  ====================
if page == "ğŸ  Home":
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### System Features")
        st.markdown("""
        - âœ¨ High Accuracy: **95.2%**
        - âš¡ High Efficiency: **96% Dimensionality Reduction**
        - ğŸ§  Multiple Methods: KNN + SVM + RF + MLP
        - ğŸ“Š Real-time Visualization
        - ğŸš€ Rapid Inference
        """)
    
    with col2:
        st.markdown("### Technical Indicators")
        st.markdown("""
        - ğŸ¯ Recognition Rate: 95%+
        - â±ï¸ Processing Time: <100ms
        - ğŸ’¾ Model Size: ~50MB
        - ğŸ–¥ï¸ Support: GPU/CPU
        - ğŸ“± Online Deployment
        """)

# ==================== æ•¸æ“šé›†ä¿¡æ¯ ====================
elif page == "ğŸ“š Dataset Info":
    st.markdown("## Dataset Information & Model Comparison")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("MNIST Training", "60,000", "images")
    with col2:
        st.metric("MNIST Testing", "10,000", "images")
    with col3:
        st.metric("Dimensions", "28Ã—28", "pixels")
    st.markdown("### Dataset Statistics")
    df_mnist = pd.DataFrame({
        'Digit': list(range(10)),
        'Training': [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949],
        'Testing': [980, 1135, 1032, 1010, 982, 892, 958, 1028, 974, 1009]
    })
    st.dataframe(df_mnist, use_container_width=True)
    st.markdown("### Fashion-MNIST Dataset")
    st.markdown("""
    - Training Set: 60,000 images
    - Testing Set: 10,000 images
    - Classes: 10 (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)
    - Format: 28Ã—28 grayscale images
    """)

    st.markdown("---")
    st.markdown("### Saved Models & Metrics Comparison")
    models_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results', 'models')
    model_files = list_saved_models(models_dir)
    if not model_files:
        st.warning("No saved models found in results/models. Train and save a model to see comparison.")
    else:
        metrics_rows = []
        for mf in model_files:
            mpath = os.path.join(models_dir, mf)
            mdl = load_saved_model(mpath)
            if mdl is not None:
                metrics = evaluate_model_on_sets(mdl, X_train, y_train, X_test, y_test)
                metrics_rows.append(metrics_to_row(mf, metrics))
        df_metrics = pd.DataFrame(metrics_rows)
        st.dataframe(df_metrics, use_container_width=True)

        st.markdown("#### Preview/Download Training-set Predictions")
        for mf in model_files:
            mpath = os.path.join(models_dir, mf)
            mdl = load_saved_model(mpath)
            if mdl is not None:
                metrics = evaluate_model_on_sets(mdl, X_train, y_train, X_test, y_test)
                preds = metrics['train']['predictions'] if metrics.get('train') else None
                if preds is not None:
                    st.markdown(f"**Model:** `{mf}`")
                    preview_df = pd.DataFrame({
                        'index': list(range(min(500, len(preds)))),
                        'true_label': y_train[:500],
                        'prediction': preds[:500]
                    })
                    st.dataframe(preview_df, use_container_width=True)
                    csv = preview_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label=f"Download training predictions ({mf})",
                        data=csv,
                        file_name=f"train_predictions_{mf.replace('.pkl','')}.csv",
                        mime='text/csv'
                    )

# ==================== ç¹ªè£½æ•¸å­— ====================
elif page == "ğŸ¨ Draw Digit":
    st.markdown("## Draw a Digit")
    st.markdown("Please draw a digit in the canvas below (0-9)")
    
    # ç°¡å–®çš„æ–‡å­—è¼¸å…¥æ–¹å¼ï¼ˆå› ç‚ºstreamlit-drawable-canvaså¯èƒ½æœ‰å•é¡Œï¼‰
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### Upload Handwritten Image")
        uploaded_file = st.file_uploader("Choose an image...", type=['jpg', 'jpeg', 'png'])
        
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            st.image(image, caption="Uploaded Image", use_container_width=True)
            
            # Add explicit button to start recognition to avoid silent failures
            if st.button('Start Recognition'):
                img_array = preprocess_image(image, size=(28, 28))
                if img_array is not None:
                    prediction, confidence, probabilities = predict_digit(model, img_array)
                    if prediction is not None:
                        col1_pred, col2_pred = st.columns([1, 2])
                        with col1_pred:
                            st.markdown(f"### Prediction: **{prediction}**")
                            st.markdown(f"Confidence: **{confidence:.2%}**")
                        with col2_pred:
                            # é¡¯ç¤ºæ‰€æœ‰æ•¸å­—çš„æ¦‚ç‡
                            fig, ax = plt.subplots(figsize=(10, 4))
                            digits = np.arange(len(probabilities)) if probabilities is not None else np.arange(10)
                            probs = probabilities if probabilities is not None else np.zeros(10)
                            ax.bar(digits, probs, color='#667eea', alpha=0.7)
                            ax.set_xlabel('Digit', fontsize=12)
                            ax.set_ylabel('Probability', fontsize=12)
                            ax.set_title('Prediction Probabilities for All Digits', fontsize=14)
                            ax.set_xticks(digits)
                            st.pyplot(fig)
    
    with col2:
        st.markdown("### Tips")
        st.markdown("""
        - Clear handwriting
        - Centered digit
        - Good contrast
        - Size: ~20Ã—20 pixels
        """)

# ==================== ä¸Šå‚³åœ–åƒ ====================
elif page == "ğŸ“¸ Upload Image":
    st.markdown("## Upload Image for Recognition")
    
    uploaded_files = st.file_uploader("Choose images", type=['jpg', 'jpeg', 'png'], accept_multiple_files=True)
    
    if uploaded_files:
        for uploaded_file in uploaded_files[:5]:  # æœ€å¤š 5 å¼µ
            col1, col2 = st.columns([1, 2])
            
            with col1:
                image = Image.open(uploaded_file)
                st.image(image, caption=uploaded_file.name, use_container_width=True)
            
            with col2:
                if st.button(f"Recognize: {uploaded_file.name}"):
                    img_array = preprocess_image(image)
                    if img_array is not None:
                        prediction, confidence, _ = predict_digit(model, img_array)
                        if prediction is not None:
                            st.markdown(f"**Digit:** {prediction}")
                            st.markdown(f"**Confidence:** {confidence:.2%}")

# ==================== æ‰¹é‡æ¸¬è©¦ ====================
elif page == "ğŸ“Š Batch Test":
    st.markdown("## Batch Test & Comparison Table")
    st.markdown("Upload images for batch recognition. Optionally upload a ground-truth CSV for comparison.")

    uploaded_files = st.file_uploader("Choose images", type=['jpg', 'jpeg', 'png'], accept_multiple_files=True)
    gt_csv = st.file_uploader("Upload ground-truth CSV (filename,label)", type=['csv'])

    gt_map = None
    if gt_csv is not None:
        gt_df = pd.read_csv(gt_csv)
        if 'filename' in gt_df.columns and 'label' in gt_df.columns:
            gt_map = dict(zip(gt_df['filename'], gt_df['label']))
        else:
            st.warning("CSV must have columns: filename,label")

    if uploaded_files:
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        for idx, uploaded_file in enumerate(uploaded_files):
            image = Image.open(uploaded_file)
            img_array = preprocess_image(image)
            pred = None
            conf = None
            if img_array is not None:
                pred, conf, _ = predict_digit(model, img_array)
            truth = gt_map.get(uploaded_file.name) if gt_map else None
            results.append({
                "filename": uploaded_file.name,
                "prediction": pred,
                "confidence": f"{conf:.2%}" if conf is not None else None,
                "truth": truth,
                "correct": (pred == truth) if (truth is not None and pred is not None) else None
            })
            progress = (idx + 1) / len(uploaded_files)
            progress_bar.progress(progress)
            status_text.text(f"Processing: {idx + 1}/{len(uploaded_files)}")
        df = pd.DataFrame(results)
        st.dataframe(df, use_container_width=True)
        st.markdown(f"âœ… Processed: {len(results)} files")
        if gt_map:
            acc = df['correct'].mean()
            st.metric("Batch Accuracy", f"{acc:.2%}")
        st.download_button(
            label="Download batch results CSV",
            data=df.to_csv(index=False).encode('utf-8'),
            file_name="batch_results.csv",
            mime='text/csv'
        )

# ==================== æ¨¡å‹è©•ä¼° ====================
elif page == "ğŸ“ˆ Model Evaluation":
    st.markdown("## Model Evaluation")
    
    if X_test is not None and y_test is not None:
        predictions = model.predict(X_test)
        
        # è©•ä¼°æŒ‡æ¨™
        accuracy = accuracy_score(y_test, predictions)
        precision = precision_score(y_test, predictions, average='weighted', zero_division=0)
        recall = recall_score(y_test, predictions, average='weighted', zero_division=0)
        f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Accuracy", f"{accuracy:.4f}")
        with col2:
            st.metric("Precision", f"{precision:.4f}")
        with col3:
            st.metric("Recall", f"{recall:.4f}")
        with col4:
            st.metric("F1 Score", f"{f1:.4f}")
        
        # æ··æ·†çŸ©é™£
        st.markdown("### Confusion Matrix")
        cm = confusion_matrix(y_test, predictions)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, 
                    xticklabels=range(10), yticklabels=range(10))
        ax.set_xlabel('Predicted Label', fontsize=12)
        ax.set_ylabel('True Label', fontsize=12)
        ax.set_title('Confusion Matrix', fontsize=14)
        st.pyplot(fig)
        
        # å„æ•¸å­—æº–ç¢ºç‡
        st.markdown("### Per-Digit Accuracy")
        per_digit_acc = []
        for digit in range(10):
            mask = y_test == digit
            if mask.sum() > 0:
                acc = accuracy_score(y_test[mask], predictions[mask])
                per_digit_acc.append(acc)
            else:
                per_digit_acc.append(0)
        
        fig, ax = plt.subplots(figsize=(12, 5))
        ax.bar(range(10), per_digit_acc, color='#667eea', alpha=0.7)
        ax.set_xlabel('Digit', fontsize=12)
        ax.set_ylabel('Accuracy', fontsize=12)
        ax.set_title('Accuracy for Each Digit', fontsize=14)
        ax.set_xticks(range(10))
        st.pyplot(fig)

# ==================== æ¨¡å‹è¨“ç·´ ====================
elif page == "ğŸ”¬ Model Training":
    st.markdown("## Model Training Process")
    
    st.markdown("""
    ### Training Pipeline (7 Steps)
    
    **Step 1: Data Loading**
    - Load MNIST dataset (60,000 training images)
    - Or use sklearn digits dataset as fallback
    - Image format: 28Ã—28 grayscale
    
    **Step 2: Feature Extraction via HOSVD**
    - Reshape 2D images into 3D tensors
    - Apply Higher-Order SVD for decomposition
    - Extract core tensor features
    - Achieve ~96% dimensionality reduction
    
    **Step 3: Compute Mean Array for Each Digit (0-9)**
    - Calculate average feature vector
    - Store reference vectors for classification
    
    **Step 4: Small-Scale Prediction Testing**
    - Use first 100 test samples
    - Compare with computed mean arrays
    - Evaluate quick prediction accuracy
    
    **Step 5: Full Test Set Evaluation**
    - Apply model to entire test set
    - Compute overall accuracy metrics
    - Generate confusion matrix
    
    **Step 6: Per-Digit Analysis**
    - Compute accuracy for each digit (0-9)
    - Identify challenging digits
    - Analyze confusion patterns
    
    **Step 7: Error Statistics**
    - Count total errors
    - Analyze error types
    - Identify most common misclassifications
    """)

# ==================== æ€§èƒ½å°æ¯” ====================
elif page == "ğŸ“Š Performance Comparison":
    st.markdown("## Model Performance Comparison")
    
    comparison_data = {
        'Model': ['KNN (K=5)', 'KNN (K=3)', 'SVM (RBF)', 'Random Forest', 'MLP', 'HOSVD+KNN'],
        'Training Time': ['Fast', 'Fast', 'Slow', 'Medium', 'Medium', 'Fast'],
        'Accuracy': ['92-94%', '93-95%', '97%+', '96-97%', '97-98%', '95%+'],
        'Memory': ['Low', 'Low', 'High', 'High', 'Medium', 'Low'],
        'Inference': ['Medium', 'Medium', 'Slow', 'Medium', 'Fast', 'Fast']
    }
    
    df_comparison = pd.DataFrame(comparison_data)
    st.dataframe(df_comparison, use_container_width=True)
    
    st.markdown("""
    ### Model Methods Explanation
    
    **KNeighborsClassifier**
    - Based on k nearest neighbors classification
    - Simple and effective for small datasets
    - Fast inference, low memory usage
    
    **Support Vector Machine (SVM)**
    - Optimal hyperplane classification
    - High accuracy on complex patterns
    - Slower training and inference
    
    **Random Forest**
    - Ensemble of decision trees
    - Balanced accuracy and speed
    - Good generalization
    
    **Multi-Layer Perceptron (MLP)**
    - Neural network with multiple layers
    - High accuracy for complex patterns
    - Fast inference after training
    
    **HOSVD+KNN**
    - Combines tensor decomposition with KNN
    - Best of both worlds
    - Excellent for handwritten digits
    """)

st.sidebar.markdown("---")
st.sidebar.markdown("**App Version:** 3.0 (Python 3.13)")
st.sidebar.markdown("**Author:** é™³å®¥èˆˆ (5114050015)")

st.sidebar.markdown("**Course:** æ•¸æ“šåˆ†ææ•¸å­¸")

# ----------------- Helpers for model management & evaluation -----------------
def list_saved_models(models_dir=None):
    models_dir = models_dir or os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results', 'models')
    if not os.path.isdir(models_dir):
        return []
    files = [f for f in os.listdir(models_dir) if f.endswith('.pkl') or f.endswith('.joblib')]
    return sorted(files)

def load_saved_model(path):
    try:
        with open(path, 'rb') as f:
            return pickle.load(f)
    except Exception:
        try:
            import joblib
            return joblib.load(path)
        except Exception:
            return None

def evaluate_model_on_sets(model, X_train, y_train, X_test, y_test):
    """Return dict with metrics and predictions for train and test sets."""
    out = {}
    if X_train is not None and y_train is not None and len(y_train) > 0:
        y_pred_train = model.predict(X_train)
        out['train'] = {
            'accuracy': float(accuracy_score(y_train, y_pred_train)),
            'precision': float(precision_score(y_train, y_pred_train, average='weighted', zero_division=0)),
            'recall': float(recall_score(y_train, y_pred_train, average='weighted', zero_division=0)),
            'f1': float(f1_score(y_train, y_pred_train, average='weighted', zero_division=0)),
            'predictions': y_pred_train
        }
    else:
        out['train'] = None

    if X_test is not None and y_test is not None and len(y_test) > 0:
        y_pred_test = model.predict(X_test)
        out['test'] = {
            'accuracy': float(accuracy_score(y_test, y_pred_test)),
            'precision': float(precision_score(y_test, y_pred_test, average='weighted', zero_division=0)),
            'recall': float(recall_score(y_test, y_pred_test, average='weighted', zero_division=0)),
            'f1': float(f1_score(y_test, y_pred_test, average='weighted', zero_division=0)),
            'predictions': y_pred_test
        }
    else:
        out['test'] = None

    return out

def make_results_dataframe(names, preds, truths=None):
    df = pd.DataFrame({
        'filename': names,
        'prediction': preds
    })
    if truths is not None:
        df['truth'] = truths
    return df

def metrics_to_row(name, metrics):
    row = {'model': name}
    if metrics.get('train'):
        row.update({
            'train_acc': metrics['train']['accuracy'],
            'train_precision': metrics['train']['precision'],
            'train_recall': metrics['train']['recall'],
            'train_f1': metrics['train']['f1']
        })
    else:
        row.update({'train_acc': None, 'train_precision': None, 'train_recall': None, 'train_f1': None})

    if metrics.get('test'):
        row.update({
            'test_acc': metrics['test']['accuracy'],
            'test_precision': metrics['test']['precision'],
            'test_recall': metrics['test']['recall'],
            'test_f1': metrics['test']['f1']
        })
    else:
        row.update({'test_acc': None, 'test_precision': None, 'test_recall': None, 'test_f1': None})

    return row

# -----------------------------------------------------------------------------

# ----------------- Helpers for model management & evaluation -----------------
def list_saved_models(models_dir=None):
    models_dir = models_dir or os.path.join(os.path.dirname(os.path.abspath(__file__)), 'results', 'models')
    if not os.path.isdir(models_dir):
        return []
    files = [f for f in os.listdir(models_dir) if f.endswith('.pkl') or f.endswith('.joblib')]
    return sorted(files)


def load_saved_model(path):
    try:
        with open(path, 'rb') as f:
            return pickle.load(f)
    except Exception:
        try:
            import joblib
            return joblib.load(path)
        except Exception:
            return None


def evaluate_model_on_sets(model, X_train, y_train, X_test, y_test):
    """Return dict with metrics and predictions for train and test sets."""
    out = {}
    if X_train is not None and y_train is not None and len(y_train) > 0:
        y_pred_train = model.predict(X_train)
        out['train'] = {
            'accuracy': float(accuracy_score(y_train, y_pred_train)),
            'precision': float(precision_score(y_train, y_pred_train, average='weighted', zero_division=0)),
            'recall': float(recall_score(y_train, y_pred_train, average='weighted', zero_division=0)),
            'f1': float(f1_score(y_train, y_pred_train, average='weighted', zero_division=0)),
            'predictions': y_pred_train
        }
    else:
        out['train'] = None

    if X_test is not None and y_test is not None and len(y_test) > 0:
        y_pred_test = model.predict(X_test)
        out['test'] = {
            'accuracy': float(accuracy_score(y_test, y_pred_test)),
            'precision': float(precision_score(y_test, y_pred_test, average='weighted', zero_division=0)),
            'recall': float(recall_score(y_test, y_pred_test, average='weighted', zero_division=0)),
            'f1': float(f1_score(y_test, y_pred_test, average='weighted', zero_division=0)),
            'predictions': y_pred_test
        }
    else:
        out['test'] = None

    return out

def make_results_dataframe(names, preds, truths=None):
    df = pd.DataFrame({
        'filename': names,
        'prediction': preds
    })
    if truths is not None:
        df['truth'] = truths
    return df

def metrics_to_row(name, metrics):
    row = {'model': name}
    if metrics.get('train'):
        row.update({
            'train_acc': metrics['train']['accuracy'],
            'train_precision': metrics['train']['precision'],
            'train_recall': metrics['train']['recall'],
            'train_f1': metrics['train']['f1']
        })
    else:
        row.update({'train_acc': None, 'train_precision': None, 'train_recall': None, 'train_f1': None})

    if metrics.get('test'):
        row.update({
            'test_acc': metrics['test']['accuracy'],
            'test_precision': metrics['test']['precision'],
            'test_recall': metrics['test']['recall'],
            'test_f1': metrics['test']['f1']
        })
    else:
        row.update({'test_acc': None, 'test_precision': None, 'test_recall': None, 'test_f1': None})

    return row

# -----------------------------------------------------------------------------
